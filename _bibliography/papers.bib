---
---

@misc{husain2024romansetu,
      title={RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models models via Romanization}, 
      author={Jaavid Aktar Husain and Raj Dabre and Aswanth Kumar and Ratish Puduppully and Anoop Kunchukuttan},
      year={2024},
      arxiv={2401.14280},
      journal  = {arXiv preprint},
      primaryClass={cs.CL}
}

@misc{zhou2024mosfad,
      title={MOS-FAD: Improving Fake Audio Detection Via Automatic Mean Opinion Score Prediction}, 
      author={Wangjin Zhou and Zhengdong Yang and Chenhui Chu and Sheng Li and Raj Dabre and Yi Zhao and Tatsuya Kawahara},
      year={2024},
      arxiv={2401.13249},
      journal  = {arXiv preprint},
      primaryClass={eess.AS}
}

@misc{chitale2024empirical,
      title={An Empirical Analysis of In-context Learning Abilities of LLMs for MT}, 
      author={Pranjal A. Chitale and Jay Gala and Varun Gumma and Mitesh M. Khapra and Raj Dabre},
      year={2024},
      arxiv={2401.12097},
      journal  = {arXiv preprint},
      primaryClass={cs.CL}
}

@misc{sravanthi2024pub,
      title={PUB: A Pragmatics Understanding Benchmark for Assessing LLMs' Pragmatics Capabilities}, 
      author={Settaluri Lakshmi Sravanthi and Meet Doshi and Tankala Pavan Kalyan and Rudra Murthy and Pushpak Bhattacharyya and Raj Dabre},
      year={2024},
      arxiv={2401.07078},
      journal  = {arXiv preprint},
      primaryClass={cs.CL}
}

@misc{joshi2024natural,
      title={Natural Language Processing for Dialects of a Language: A Survey}, 
      author={Aditya Joshi and Raj Dabre and Diptesh Kanojia and Zhuang Li and Haolan Zhan and Gholamreza Haffari and Doris Dippold},
      year={2024},
      arxiv={2401.05632},
      journal  = {arXiv preprint},
      primaryClass={cs.CL}
}

@misc{lent2023creoleval,
      title={CreoleVal: Multilingual Multitask Benchmarks for Creoles}, 
      author={Heather Lent and Kushal Tatariya and Raj Dabre and Yiyi Chen and Marcell Fekete and Esther Ploeger and Li Zhou and Hans Erik Heje and Diptesh Kanojia and Paul Belony and Marcel Bollmann and Loïc Grobol and Miryam de Lhoneux and Daniel Hershcovich and Michel DeGraff and Anders Søgaard and Johannes Bjerva},
      year={2023},
      arxiv={2310.19567},
      journal  = {arXiv preprint},
      primaryClass={cs.CL}
}

@article{10.1145/3610611,
author = {Song, Haiyue and Dabre, Raj and Chu, Chenhui and Kurohashi, Sadao and Sumita, Eiichiro},
title = {SelfSeg: A Self-supervised Sub-word Segmentation Method for Neural Machine Translation},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {8},
issn = {2375-4699},
html = {https://doi.org/10.1145/3610611},
doi = {10.1145/3610611},
abstract = {Sub-word segmentation is an essential pre-processing step for Neural Machine Translation (NMT). Existing work has shown that neural sub-word segmenters are better than Byte-Pair Encoding (BPE), however, they are inefficient, as they require parallel corpora, days to train, and hours to decode. This article introduces SelfSeg, a self-supervised neural sub-word segmentation method that is much faster to train/decode and requires only monolingual dictionaries instead of parallel corpora. SelfSeg takes as input a word in the form of a partially masked character sequence, optimizes the word generation probability, and generates the segmentation with the maximum posterior probability, which is calculated using a dynamic programming algorithm. The training time of SelfSeg depends on word frequencies, and we explore several word frequency normalization strategies to accelerate the training phase. Additionally, we propose a regularization mechanism that allows the segmenter to generate various segmentations for one word. To show the effectiveness of our approach, we conduct MT experiments in low-, middle-, and high-resource scenarios, where we compare the performance of using different segmentation methods. The experimental results demonstrate that, on the low-resource ALT dataset, our method achieves more than 1.2 BLEU score improvement compared with BPE and SentencePiece, and a 1.1 score improvement over Dynamic Programming Encoding (DPE) and Vocabulary Learning via Optimal Transport (VOLT), on average. The regularization method achieves approximately a 4.3 BLEU score improvement over BPE and a 1.2 BLEU score improvement over BPE-dropout, the regularized version of BPE. We also observed significant improvements on IWSLT15 Vi→En, WMT16 Ro→En, and WMT15 Fi→En datasets and competitive results on the WMT14 De→En and WMT14 Fr→En datasets. Furthermore, our method is 17.8\texttimes{} faster during training and up to 36.8\texttimes{} faster during decoding in a high-resource scenario compared to DPE. We provide extensive analysis, including why monolingual word-level data is enough to train SelfSeg.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {aug},
articleno = {215},
numpages = {24},
keywords = {Subword segmentation, self-supervised learning, machine translation, efficient NLP, subword regularization}
}

@inproceedings{
ganesan2023supershaper,
title={SuperShaper: A Pre-Training Approach for Discovering Efficient Transformer Shapes},
author={Vinod Ganesan and Gowtham Ramesh and Pratyush Kumar and Raj Dabre},
booktitle={Workshop on Efficient Systems for Foundation Models @ ICML2023},
year={2023},
url={https://openreview.net/forum?id=SESK4eCQVs}
}

@misc{yang2023scicap,
      title={SciCap+: A Knowledge Augmented Dataset to Study the Challenges of Scientific Figure Captioning}, 
      author={Zhishen Yang and Raj Dabre and Hideki Tanaka and Naoaki Okazaki},
      year={2023},
      arxiv={2306.03491},
      journal  = {arXiv preprint},
      primaryClass={cs.CV}
}

@article{10.1145/3594631,
author = {Chakrabarty, Abhisek and Dabre, Raj and Ding, Chenchen and Utiyama, Masao and Sumita, Eiichiro},
title = {Low-resource Multilingual Neural Translation Using Linguistic Feature-based Relevance Mechanisms},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {7},
issn = {2375-4699},
html = {https://doi.org/10.1145/3594631},
doi = {10.1145/3594631},
abstract = {This article investigates approaches to effectively harness source-side linguistic features for low-resource multilingual neural machine translation (MNMT). Previous works focus on using various features of a word such as lemma, part-of-speech tag, dependency label, and so on, to improve translation quality in a low-resource scenario. However, these studies deal with bilingual translation and do not focus on using features in multilingual training setups. Our work focuses on this particular point and experiments with low-resource multilingual models incorporating source-side linguistic features. Although techniques for integrating features into an NMT model such as concatenation and feature relevance perform quite well in bilingual settings, they do not work well in multilingual settings. To remedy this, we propose the use of dummy features and language indicator features in MNMT models. Experiments are conducted on English to Asian language translation on a multilingual, multi-parallel corpus spanning English and eight Asian languages where for each language pair, the training data size does not exceed 20,000 parallel sentences. After establishing strong bilingual baselines using feature relevance mechanisms and multilingual baselines without any features, we show that our proposed dummy features and language indicator features, in combination with feature relevance mechanisms, yield significant improvements in BLEU points for all language pairs. We then analyze our models from the perspectives of model sizes, the impact of individual linguistic features, validation perplexity computed during training, visualization of the activations of the relevance mechanisms, and exhaustive tuning of hyperparameters. We also report preliminary results for multilingual multi-way models using linguistic features.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jul},
articleno = {191},
numpages = {36},
keywords = {Linguistic features, morphology, neural networks}
}

@article{gala2024airavata,
  abbr     = {arXiv},
  title    = {Airavata: Introducing Hindi Instruction-tuned LLM},
  author   = {Jay Gala and Thanmay Jayakumar and Jaavid Aktar Husain and Aswanth Kumar M and Mohammed Safi Ur Rahman Khan and Diptesh Kanojia and Ratish Puduppully and Mitesh M. Khapra and Raj Dabre and Rudra Murthy and Anoop Kunchukuttan},
  abstract = {We announce the initial release of Airavata, an instruction-tuned LLM for Hindi. Airavata was created by fine-tuning OpenHathi with diverse, instruction-tuning Hindi datasets to make it better suited for assistive tasks. Along with the model, we also share the IndicInstruct dataset, which is a collection of diverse instruction-tuning datasets to enable further research for Indic LLMs. Additionally, we present evaluation benchmarks and a framework for assessing LLM performance across tasks in Hindi. Currently, Airavata supports Hindi, but we plan to expand this to all 22 scheduled Indic languages. You can access all artifacts at https://ai4bharat.github.io/airavata.},
  year     = {2024},
  journal  = {arXiv preprint},
  arxiv    = {2401.15006},
  html     = {https://ai4bharat.github.io/airavata},
  code     = {https://github.com/AI4Bharat/IndicInstruct}
}

@inproceedings{gala2023indictrans,
  abbr     = {TMLR},
  title    = {IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages},
  author   = {Jay Gala* and Pranjal A. Chitale* and Raghavan AK and Varun Gumma and Sumanth Doddapaneni and Aswanth Kumar and Janki Nawale and Anupama Sujatha and Ratish Puduppully and Vivek Raghavan and Pratyush Kumar and Mitesh M. Khapra and Raj Dabre and Anoop Kunchukuttan},
  abstract = {India has a rich linguistic landscape with languages from 4 major language families spoken by over a billion people. 22 of these languages are listed in the Constitution of India (referred to as scheduled languages) are the focus of this work. Given the linguistic diversity, high-quality and accessible Machine Translation (MT) systems are essential in a country like India. Prior to this work, there was (i) no parallel training data spanning all the 22 languages, (ii) no robust benchmarks covering all these languages and containing content relevant to India, and (iii) no existing translation models which support all the 22 scheduled languages of India. In this work, we aim to address this gap by focusing on the missing pieces required for enabling wide, easy, and open access to good machine translation systems for all 22 scheduled Indian languages. We identify four key areas of improvement: curating and creating larger training datasets, creating diverse and high-quality benchmarks, training multilingual models, and releasing models with open access. Our first contribution is the release of the Bharat Parallel Corpus Collection (BPCC), the largest publicly available parallel corpora for Indic languages. BPCC contains a total of 230M bitext pairs, of which a total of 126M were newly added, including 644K manually translated sentence pairs created as part of this work. Our second contribution is the release of the first n-way parallel benchmark covering all 22 Indian languages, featuring diverse domains, Indian-origin content, and source-original test sets. Next, we present IndicTrans2, the first model to support all 22 languages, surpassing existing models on multiple existing and new benchmarks created as a part of this work. Lastly, to promote accessibility and collaboration, we release our models and associated data with permissive licenses at https://github.com/ai4bharat/indictrans2.},
  year     = {2023},
  journal  = {Transactions on Machine Learning Research},
  arxiv    = {2305.16307},
  code     = {https://github.com/ai4bharat/indictrans2},
  html     = {https://openreview.net/forum?id=vfT4YuzAYA}
}



@inproceedings{gumma-etal-2023-empirical,
    title = "An Empirical Study of Leveraging Knowledge Distillation for Compressing Multilingual Neural Machine Translation Models",
    author = "Gumma, Varun  and
      Dabre, Raj  and
      Kumar, Pratyush",
    editor = "Nurminen, Mary  and
      Brenner, Judith  and
      Koponen, Maarit  and
      Latomaa, Sirkku  and
      Mikhailov, Mikhail  and
      Schierl, Frederike  and
      Ranasinghe, Tharindu  and
      Vanmassenhove, Eva  and
      Vidal, Sergi Alvarez  and
      Aranberri, Nora  and
      Nunziatini, Mara  and
      Escart{\'\i}n, Carla Parra  and
      Forcada, Mikel  and
      Popovic, Maja  and
      Scarton, Carolina  and
      Moniz, Helena",
    booktitle = "Proceedings of the 24th Annual Conference of the European Association for Machine Translation",
    month = jun,
    year = "2023",
    address = "Tampere, Finland",
    publisher = "European Association for Machine Translation",
    html = "https://aclanthology.org/2023.eamt-1.11",
    pages = "103--114",
    abstract = "Knowledge distillation (KD) is a well-known method for compressing neural models. However, works focusing on distilling knowledge from large multilingual neural machine translation (MNMT) models into smaller ones are practically nonexistent, despite the popularity and superiority of MNMT. This paper bridges this gap by presenting an empirical investigation of knowledge distillation for compressing MNMT models. We take Indic to English translation as a case study and demonstrate that commonly used language-agnostic and language-aware KD approaches yield models that are 4-5x smaller but also suffer from performance drops of up to 3.5 BLEU. To mitigate this, we then experiment with design considerations such as shallower versus deeper models, heavy parameter sharing, multistage training, and adapters. We observe that deeper compact models tend to be as good as shallower non-compact ones and that fine-tuning a distilled model on a high-quality subset slightly boosts translation quality. Overall, we conclude that compressing MNMT models via KD is challenging, indicating immense scope for further research.",
}

@inproceedings{sai-b-etal-2023-indicmt,
    title = "{I}ndic{MT} Eval: A Dataset to Meta-Evaluate Machine Translation Metrics for {I}ndian Languages",
    author = "Sai B, Ananya  and
      Dixit, Tanay  and
      Nagarajan, Vignesh  and
      Kunchukuttan, Anoop  and
      Kumar, Pratyush  and
      Khapra, Mitesh M.  and
      Dabre, Raj",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2023.acl-long.795",
    doi = "10.18653/v1/2023.acl-long.795",
    pages = "14210--14228",
    abstract = "The rapid growth of machine translation (MT) systems necessitates meta-evaluations of evaluation metrics to enable selection of those that best reflect MT quality. Unfortunately, most meta-evaluation studies focus on European languages, the observations for which may not always apply to other languages. Indian languages, having over a billion speakers, are linguistically different from them, and to date, there are no such systematic studies focused solely on English to Indian language MT. This paper fills this gap through a Multidimensional Quality Metric (MQM) dataset consisting of 7000 fine-grained annotations, spanning 5 Indian languages and 7 MT systems. We evaluate 16 metrics and show that, pre-trained metrics like COMET have the highest correlations with annotator scores as opposed to n-gram metrics like BLEU. We further leverage our MQM annotations to develop an Indic-COMET metric and show that it outperforms COMET counterparts in both human scores correlations and robustness scores in Indian languages. Additionally, we show that the Indic-COMET can outperform COMET on some unseen Indian languages. We hope that our dataset and analysis will facilitate further research in Indic MT evaluation.",
}

@inproceedings{mao-etal-2023-exploring,
    title = "Exploring the Impact of Layer Normalization for Zero-shot Neural Machine Translation",
    author = "Mao, Zhuoyuan  and
      Dabre, Raj  and
      Liu, Qianying  and
      Song, Haiyue  and
      Chu, Chenhui  and
      Kurohashi, Sadao",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2023.acl-short.112",
    doi = "10.18653/v1/2023.acl-short.112",
    pages = "1300--1316",
    abstract = "This paper studies the impact of layer normalization (LayerNorm) on zero-shot translation (ZST). Recent efforts for ZST often utilize the Transformer architecture as the backbone, with LayerNorm at the input of layers (PreNorm) set as the default. However, Xu et al. (2019) has revealed that PreNorm carries the risk of overfitting the training data. Based on this, we hypothesize that PreNorm may overfit supervised directions and thus have low generalizability for ZST. Through experiments on OPUS, IWSLT, and Europarl datasets for 54 ZST directions, we demonstrate that the original Transformer setting of LayerNorm after residual connections (PostNorm) consistently outperforms PreNorm by up to 12.3 BLEU points. We then study the performance disparities by analyzing the differences in off-target rates and structural variations between PreNorm and PostNorm. This study highlights the need for careful consideration of the LayerNorm setting for ZST.",
}

@inproceedings{dabre-etal-2023-yanmtt,
    title = "{YANMTT}: Yet Another Neural Machine Translation Toolkit",
    author = "Dabre, Raj  and
      Kanojia, Diptesh  and
      Sawant, Chinmay  and
      Sumita, Eiichiro",
    editor = "Bollegala, Danushka  and
      Huang, Ruihong  and
      Ritter, Alan",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2023.acl-demo.24",
    doi = "10.18653/v1/2023.acl-demo.24",
    pages = "257--263",
    abstract = "In this paper, we present our open-source neural machine translation (NMT) toolkit called {``}Yet Another Neural Machine Translation Toolkit{''} abbreviated as YANMTT - \url{https://github.com/prajdabre/yanmtt}, which is built on top of the HuggingFace Transformers library. YANMTT focuses on transfer learning and enables easy pre-training and fine-tuning of sequence-to-sequence models at scale. It can be used for training parameter-heavy models with minimal parameter sharing and efficient, lightweight models via heavy parameter sharing. Additionally, it supports parameter-efficient fine-tuning (PEFT) through adapters and prompts. Our toolkit also comes with a user interface that can be used to demonstrate these models and visualize various parts of the model. Apart from these core features, our toolkit also provides other advanced functionalities such as but not limited to document/multi-source NMT, simultaneous NMT, mixtures-of-experts, model compression and continual learning.",
}

@inproceedings{puduppully-etal-2023-decomt,
    title = "{D}eco{MT}: Decomposed Prompting for Machine Translation Between Related Languages using Large Language Models",
    author = "Puduppully, Ratish  and
      Kunchukuttan, Anoop  and
      Dabre, Raj  and
      Aw, Ai Ti  and
      Chen, Nancy",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2023.emnlp-main.279",
    doi = "10.18653/v1/2023.emnlp-main.279",
    pages = "4586--4602",
    abstract = "This study investigates machine translation between related languages i.e., languages within the same family that share linguistic characteristics such as word order and lexical similarity. Machine translation through few-shot prompting leverages a small set of translation pair examples to generate translations for test sentences. This procedure requires the model to learn how to generate translations while simultaneously ensuring that token ordering is maintained to produce a fluent and accurate translation. We propose that for related languages, the task of machine translation can be simplified by leveraging the monotonic alignment characteristic of such languages. We introduce DecoMT, a novel approach of few-shot prompting that decomposes the translation process into a sequence of word chunk translations. Through automatic and human evaluation conducted on multiple related language pairs across various language families, we demonstrate that our proposed approach of decomposed prompting surpasses multiple established few-shot baseline approaches. For example, DecoMT outperforms the strong few-shot prompting BLOOM model with an average improvement of 8 chrF++ scores across the examined languages.",
}

@proceedings{wat-2023-asian,
    title = "Proceedings of the 10th Workshop on Asian Translation",
    editor = "Nakazawa, Toshiaki  and
      Kinugawa, Kazutaka  and
      Mino, Hideya  and
      Goto, Isao  and
      Dabre, Raj  and
      Higashiyama, Shohei  and
      Parida, Shantipriya  and
      Morishita, Makoto  and
      Bojar, Ondrej  and
      Eriguchi, Akiko  and
      Oda, Yusuke  and
      Eriguchi, Akiko  and
      Chu, Chenhui  and
      Kurohashi, Sadao",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    html = "https://aclanthology.org/2023.wat-1.0",
}

@inproceedings{nakazawa-etal-2023-overview,
    title = "Overview of the 10th Workshop on {A}sian Translation",
    author = "Nakazawa, Toshiaki  and
      Kinugawa, Kazutaka  and
      Mino, Hideya  and
      Goto, Isao  and
      Dabre, Raj  and
      Higashiyama, Shohei  and
      Parida, Shantipriya  and
      Morishita, Makoto  and
      Bojar, Ond{\v{r}}ej  and
      Eriguchi, Akiko  and
      Oda, Yusuke  and
      Chu, Chenhui  and
      Kurohashi, Sadao",
    editor = "Nakazawa, Toshiaki  and
      Kinugawa, Kazutaka  and
      Mino, Hideya  and
      Goto, Isao  and
      Dabre, Raj  and
      Higashiyama, Shohei  and
      Parida, Shantipriya  and
      Morishita, Makoto  and
      Bojar, Ondrej  and
      Eriguchi, Akiko  and
      Oda, Yusuke  and
      Eriguchi, Akiko  and
      Chu, Chenhui  and
      Kurohashi, Sadao",
    booktitle = "Proceedings of the 10th Workshop on Asian Translation",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    html = "https://aclanthology.org/2023.wat-1.1",
    pages = "1--28",
    abstract = "This paper presents the results of the shared tasks from the 10th workshop on Asian translation (WAT2023). For the WAT2023, 2 teams submitted their translation results for the human evaluation. We also accepted 1 research paper. About 40 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated.",
}

@inproceedings{machacek-etal-2023-mt,
    title = "{MT} Metrics Correlate with Human Ratings of Simultaneous Speech Translation",
    author = "Mach{\'a}{\v{c}}ek, Dominik  and
      Bojar, Ond{\v{r}}ej  and
      Dabre, Raj",
    editor = "Salesky, Elizabeth  and
      Federico, Marcello  and
      Carpuat, Marine",
    booktitle = "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada (in-person and online)",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2023.iwslt-1.12",
    doi = "10.18653/v1/2023.iwslt-1.12",
    pages = "169--179",
    abstract = "There have been several meta-evaluation studies on the correlation between human ratings and offline machine translation (MT) evaluation metrics such as BLEU, chrF2, BertScore and COMET. These metrics have been used to evaluate simultaneous speech translation (SST) but their correlations with human ratings of SST, which has been recently collected as Continuous Ratings (CR), are unclear. In this paper, we leverage the evaluations of candidate systems submitted to the English-German SST task at IWSLT 2022 and conduct an extensive correlation analysis of CR and the aforementioned metrics. Our study reveals that the offline metrics are well correlated with CR and can be reliably used for evaluating machine translation in simultaneous mode, with some limitations on the test set size. We conclude that given the current quality levels of SST, these metrics can be used as proxies for CR, alleviating the need for large scale human evaluation. Additionally, we observe that correlations of the metrics with translation as a reference is significantly higher than with simultaneous interpreting, and thus we recommend the former for reliable evaluation.",
}

@inproceedings{dabre-etal-2023-nict,
    title = "{NICT}-{AI}4{B}{'}s Submission to the {I}ndic {MT} Shared Task in {WMT} 2023",
    author = "Dabre, Raj  and
      Gala, Jay  and
      Chitale, Pranjal",
    editor = "Koehn, Philipp  and
      Haddow, Barry  and
      Kocmi, Tom  and
      Monz, Christof",
    booktitle = "Proceedings of the Eighth Conference on Machine Translation",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2023.wmt-1.88",
    doi = "10.18653/v1/2023.wmt-1.88",
    pages = "941--949",
    abstract = "In this paper, we (Team NICT-AI4B) describe our MT systems that we submit to the Indic MT task in WMT 2023. Our primary system consists of 3 stages: Joint denoising and MT training using officially approved monolingual and parallel corpora, backtranslation and, MT training on original and backtranslated parallel corpora. We observe that backtranslation leads to substantial improvements in translation quality up to 4 BLEU points. We also develop 2 contrastive systems on unconstrained settings, where the first system involves fine-tuning of IndicTrans2 DA models on official parallel corpora and seed data used in AI4Bharat et al, (2023), and the second system involves a system combination of the primary and the aforementioned system. Overall, we manage to obtain high-quality translation systems for the 4 low-resource North-East Indian languages of focus.",
}

@inproceedings{machacek-etal-2023-robustness,
    title = "Robustness of Multi-Source {MT} to Transcription Errors",
    author = "Mach{\'a}{\v{c}}ek, Dominik  and
      Pol{\'a}k, Peter  and
      Bojar, Ond{\v{r}}ej  and
      Dabre, Raj",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2023.findings-acl.228",
    doi = "10.18653/v1/2023.findings-acl.228",
    pages = "3707--3723",
    abstract = "Automatic speech translation is sensitive to speech recognition errors, but in a multilingual scenario, the same content may be available in various languages via simultaneous interpreting, dubbing or subtitling. In this paper, we hypothesize that leveraging multiple sources will improve translation quality if the sources complement one another in terms of correct information they contain. To this end, we first show that on a 10-hour ESIC corpus, the ASR errors in the original English speech and its simultaneous interpreting into German and Czech are mutually independent. We then use two sources, English and German, in a multi-source setting for translation into Czech to establish its robustness to ASR errors. Furthermore, we observe this robustness when translating both noisy sources together in a simultaneous translation setting. Our results show that multi-source neural machine translation has the potential to be useful in a real-time simultaneous translation setting, thereby motivating further investigation in this area.",
}

@inproceedings{m-etal-2023-ctqscorer,
    title = "{CTQS}corer: Combining Multiple Features for In-context Example Selection for Machine Translation",
    author = "M, Aswanth  and
      Puduppully, Ratish  and
      Dabre, Raj  and
      Kunchukuttan, Anoop",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2023.findings-emnlp.519",
    doi = "10.18653/v1/2023.findings-emnlp.519",
    pages = "7736--7752",
    abstract = "Large language models have demonstrated the capability to perform on machine translation when the input is prompted with a few examples (in-context learning). Translation quality depends on various features of the selected examples, such as their quality and relevance, but previous work has predominantly focused on individual features in isolation. In this paper, we propose a general framework for combining different features influencing example selection. We learn a regression model, CTQ Scorer (Contextual Translation Quality), that selects examples based on multiple features in order to maximize the translation quality. On multiple language pairs and language models, we show that CTQ Scorer helps significantly outperform random selection as well as strong single-factor baselines reported in the literature. We also see an improvement of over 2.5 COMET points on average with respect to a strong BM25 retrieval-based baseline.",
}

@inproceedings{dabre-etal-2023-study,
    title = "A Study on the Effectiveness of Large Language Models for Translation with Markup",
    author = "Dabre, Raj  and
      Buschbeck, Bianka  and
      Exel, Miriam  and
      Tanaka, Hideki",
    editor = "Utiyama, Masao  and
      Wang, Rui",
    booktitle = "Proceedings of Machine Translation Summit XIX, Vol. 1: Research Track",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    html = "https://aclanthology.org/2023.mtsummit-research.13",
    pages = "148--159",
    abstract = "In this paper we evaluate the utility of large language models (LLMs) for translation of text with markup in which the most important and challenging aspect is to correctly transfer markup tags while ensuring that the content, both, inside and outside tags is correctly translated. While LLMs have been shown to be effective for plain text translation, their effectiveness for structured document translation is not well understood. To this end, we experiment with BLOOM and BLOOMZ, which are open-source multilingual LLMs, using zero, one and few-shot prompting, and compare with a domain-specific in-house NMT system using a detag-and-project approach for markup tags. We observe that LLMs with in-context learning exhibit poorer translation quality compared to the domain-specific NMT system, however, they are effective in transferring markup tags, especially the large BLOOM model (176 billion parameters). This is further confirmed by our human evaluation which also reveals the types of errors of the different tag transfer techniques. While LLM-based approaches come with the risk of losing, hallucinating and corrupting tags, they excel at placing them correctly in the translation.",
}

@inproceedings{mao-etal-2023-variable,
    title = "Variable-length Neural Interlingua Representations for Zero-shot Neural Machine Translation",
    author = "Mao, Zhuoyuan  and
      Song, Haiyue  and
      Dabre, Raj  and
      Chu, Chenhui  and
      Kurohashi, Sadao",
    editor = "Barreiro, Anabela  and
      Silberztein, Max  and
      Lloret, Elena  and
      Paprzycki, Marcin",
    booktitle = "Proceedings of the 1st International Workshop on Multilingual, Multimodal and Multitask Language Generation",
    month = jun,
    year = "2023",
    address = "Tampere, Finland",
    publisher = "European Association for Machine Translation",
    html = "https://aclanthology.org/2023.multi3generation-1.3",
    pages = "16--25",
}

@inproceedings{song-etal-2022-bertseg,
    title = "{BERTS}eg: {BERT} Based Unsupervised Subword Segmentation for Neural Machine Translation",
    author = "Song, Haiyue  and
      Dabre, Raj  and
      Mao, Zhuoyuan  and
      Chu, Chenhui  and
      Kurohashi, Sadao",
    editor = "He, Yulan  and
      Ji, Heng  and
      Li, Sujian  and
      Liu, Yang  and
      Chang, Chua-Hui",
    booktitle = "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = nov,
    year = "2022",
    address = "Online only",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2022.aacl-short.12",
    pages = "85--94",
    abstract = "Existing subword segmenters are either 1) frequency-based without semantics information or 2) neural-based but trained on parallel corpora. To address this, we present BERTSeg, an unsupervised neural subword segmenter for neural machine translation, which utilizes the contextualized semantic embeddings of words from characterBERT and maximizes the generation probability of subword segmentations. Furthermore, we propose a generation probability-based regularization method that enables BERTSeg to produce multiple segmentations for one word to improve the robustness of neural machine translation. Experimental results show that BERTSeg with regularization achieves up to 8 BLEU points improvement in 9 translation directions on ALT, IWSLT15 Vi-{\textgreater}En, WMT16 Ro-{\textgreater}En, and WMT15 Fi-{\textgreater}En datasets compared with BPE. In addition, BERTSeg is efficient, needing up to 5 minutes for training.",
}

@inproceedings{chakrabarty-etal-2022-featurebart,
    title = "{F}eature{BART}: Feature Based Sequence-to-Sequence Pre-Training for Low-Resource {NMT}",
    author = "Chakrabarty, Abhisek  and
      Dabre, Raj  and
      Ding, Chenchen  and
      Tanaka, Hideki  and
      Utiyama, Masao  and
      Sumita, Eiichiro",
    editor = "Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    html = "https://aclanthology.org/2022.coling-1.443",
    pages = "5014--5020",
    abstract = "In this paper we present FeatureBART, a linguistically motivated sequence-to-sequence monolingual pre-training strategy in which syntactic features such as lemma, part-of-speech and dependency labels are incorporated into the span prediction based pre-training framework (BART). These automatically extracted features are incorporated via approaches such as concatenation and relevance mechanisms, among which the latter is known to be better than the former. When used for low-resource NMT as a downstream task, we show that these feature based models give large improvements in bilingual settings and modest ones in multilingual settings over their counterparts that do not use features.",
}

@inproceedings{dabre-2022-nict,
    title = "{NICT} at {M}ix{MT} 2022: Synthetic Code-Mixed Pre-training and Multi-way Fine-tuning for {H}inglish{--}{E}nglish Translation",
    author = "Dabre, Raj",
    editor = {Koehn, Philipp  and
      Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Jimeno Yepes, Antonio  and
      Kocmi, Tom  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Popel, Martin  and
      Turchi, Marco  and
      Zampieri, Marcos},
    booktitle = "Proceedings of the Seventh Conference on Machine Translation (WMT)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2022.wmt-1.111",
    pages = "1122--1125",
    abstract = "In this paper, we describe our submission to the Code-mixed Machine Translation (MixMT) shared task. In MixMT, the objective is to translate Hinglish to English and vice versa. For our submissions, we focused on code-mixed pre-training and multi-way fine-tuning. Our submissions achieved rank 4 in terms of automatic evaluation score. For Hinglish to English translation, our submission achieved rank 4 as well.",
}

@inproceedings{kumar-etal-2022-indicnlg,
    title = "{I}ndic{NLG} Benchmark: Multilingual Datasets for Diverse {NLG} Tasks in {I}ndic Languages",
    author = "Kumar, Aman  and
      Shrotriya, Himani  and
      Sahu, Prachi  and
      Mishra, Amogh  and
      Dabre, Raj  and
      Puduppully, Ratish  and
      Kunchukuttan, Anoop  and
      Khapra, Mitesh M.  and
      Kumar, Pratyush",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2022.emnlp-main.360",
    doi = "10.18653/v1/2022.emnlp-main.360",
    pages = "5363--5394",
    abstract = "Natural Language Generation (NLG) for non-English languages is hampered by the scarcity of datasets in these languages. We present the IndicNLG Benchmark, a collection of datasets for benchmarking NLG for 11 Indic languages. We focus on five diverse tasks, namely, biography generation using Wikipedia infoboxes, news headline generation, sentence summarization, paraphrase generation and, question generation. We describe the created datasets and use them to benchmark the performance of several monolingual and multilingual baselines that leverage pre-trained sequence-to-sequence models. Our results exhibit the strong performance of multilingual language-specific pre-trained models, and the utility of models trained on our dataset for other related NLG tasks. Our dataset creation methods can be easily applied to modest-resource languages as they involve simple steps such as scraping news articles and Wikipedia infoboxes, light cleaning, and pivoting through machine translation data. To the best of our knowledge, the IndicNLG Benchmark is the first NLG benchmark for Indic languages and the most diverse multilingual NLG dataset, with approximately 8M examples across 5 tasks and 11 languages. The datasets and models will be publicly available.",
}

@inproceedings{nakazawa-etal-2022-overview,
    title = "Overview of the 9th Workshop on {A}sian Translation",
    author = "Nakazawa, Toshiaki  and
      Mino, Hideya  and
      Goto, Isao  and
      Dabre, Raj  and
      Higashiyama, Shohei  and
      Parida, Shantipriya  and
      Kunchukuttan, Anoop  and
      Morishita, Makoto  and
      Bojar, Ond{\v{r}}ej  and
      Chu, Chenhui  and
      Eriguchi, Akiko  and
      Abe, Kaori  and
      Oda, Yusuke  and
      Kurohashi, Sadao",
    booktitle = "Proceedings of the 9th Workshop on Asian Translation",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Conference on Computational Linguistics",
    html = "https://aclanthology.org/2022.wat-1.1",
    pages = "1--36",
    abstract = "This paper presents the results of the shared tasks from the 9th workshop on Asian translation (WAT2022). For the WAT2022, 8 teams submitted their translation results for the human evaluation. We also accepted 4 research papers. About 300 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated.",
}

@inproceedings{dabre-2022-nicts,
    title = "{NICT}{'}s Submission to the {WAT} 2022 Structured Document Translation Task",
    author = "Dabre, Raj",
    booktitle = "Proceedings of the 9th Workshop on Asian Translation",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Conference on Computational Linguistics",
    html = "https://aclanthology.org/2022.wat-1.6",
    pages = "64--67",
    abstract = "We present our submission to the structured document translation task organized by WAT 2022. In structured document translation, the key challenge is the handling of inline tags, which annotate text. Specifically, the text that is annotated by tags, should be translated in such a way that in the translation should contain the tags annotating the translation. This challenge is further compounded by the lack of training data containing sentence pairs with inline XML tag annotated content. However, to our surprise, we find that existing multilingual NMT systems are able to handle the translation of text annotated with XML tags without any explicit training on data containing said tags. Specifically, massively multilingual translation models like M2M-100 perform well despite not being explicitly trained to handle structured content. This direct translation approach is often either as good as if not better than the traditional approach of {``}remove tag, translate and re-inject tag{''} also known as the {``}detag-and-project{''} approach.",
}

@inproceedings{dabre-etal-2022-indicbart,
    title = "{I}ndic{BART}: A Pre-trained Model for Indic Natural Language Generation",
    author = "Dabre, Raj  and
      Shrotriya, Himani  and
      Kunchukuttan, Anoop  and
      Puduppully, Ratish  and
      Khapra, Mitesh  and
      Kumar, Pratyush",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2022.findings-acl.145",
    doi = "10.18653/v1/2022.findings-acl.145",
    pages = "1849--1863",
    abstract = "In this paper, we study pre-trained sequence-to-sequence models for a group of related languages, with a focus on Indic languages. We present IndicBART, a multilingual, sequence-to-sequence pre-trained model focusing on 11 Indic languages and English. IndicBART utilizes the orthographic similarity between Indic scripts to improve transfer learning between similar Indic languages. We evaluate IndicBART on two NLG tasks: Neural Machine Translation (NMT) and extreme summarization. Our experiments on NMT and extreme summarization show that a model specific to related languages like IndicBART is competitive with large pre-trained models like mBART50 despite being significantly smaller. It also performs well on very low-resource translation scenarios where languages are not included in pre-training or fine-tuning. Script sharing, multilingual training, and better utilization of limited model capacity contribute to the good performance of the compact IndicBART model.",
}

@inproceedings{mao-etal-2022-contrastive,
    title = "When do Contrastive Word Alignments Improve Many-to-many Neural Machine Translation?",
    author = "Mao, Zhuoyuan  and
      Chu, Chenhui  and
      Dabre, Raj  and
      Song, Haiyue  and
      Wan, Zhen  and
      Kurohashi, Sadao",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2022.findings-naacl.134",
    doi = "10.18653/v1/2022.findings-naacl.134",
    pages = "1766--1775",
    abstract = "Word alignment has proven to benefit many-to-many neural machine translation (NMT). However, high-quality ground-truth bilingual dictionaries were used for pre-editing in previous methods, which are unavailable for most language pairs. Meanwhile, the contrastive objective can implicitly utilize automatically learned word alignment, which has not been explored in many-to-many NMT. This work proposes a word-level contrastive objective to leverage word alignments for many-to-many NMT. Empirical results show that this leads to 0.8 BLEU gains for several language pairs. Analyses reveal that in many-to-many NMT, the encoder{'}s sentence retrieval performance highly correlates with the translation quality, which explains when the proposed method impacts translation. This motivates future exploration for many-to-many NMT to improve the encoder{'}s sentence retrieval performance.",
}

@inproceedings{dabre-sukhoo-2022-kreolmorisienmt,
    title = "{K}reol{M}orisien{MT}: A Dataset for Mauritian Creole Machine Translation",
    author = "Dabre, Raj  and
      Sukhoo, Aneerav",
    editor = "He, Yulan  and
      Ji, Heng  and
      Li, Sujian  and
      Liu, Yang  and
      Chang, Chua-Hui",
    booktitle = "Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022",
    month = nov,
    year = "2022",
    address = "Online only",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2022.findings-aacl.3",
    pages = "22--29",
    abstract = "In this paper, we describe KreolMorisienMT, a dataset for benchmarking machine translation quality of Mauritian Creole. Mauritian Creole (Kreol Morisien) is a French-based creole and a lingua franca of the Republic of Mauritius. KreolMorisienMT consists of a parallel corpus between English and Kreol Morisien, French and Kreol Morisien and a monolingual corpus for Kreol Morisien. We first give an overview of Kreol Morisien and then describe the steps taken to create the corpora. Thereafter, we benchmark Kreol Morisien ↔ English and Kreol Morisien ↔ French models leveraging pre-trained models and multilingual transfer learning. Human evaluation reveals our systems{'} high translation quality.",
}

@inproceedings{buschbeck-etal-2022-multilingual,
    title = "A Multilingual Multiway Evaluation Data Set for Structured Document Translation of {A}sian Languages",
    author = "Buschbeck, Bianka  and
      Dabre, Raj  and
      Exel, Miriam  and
      Huck, Matthias  and
      Huy, Patrick  and
      Rubino, Raphael  and
      Tanaka, Hideki",
    editor = "He, Yulan  and
      Ji, Heng  and
      Li, Sujian  and
      Liu, Yang  and
      Chang, Chua-Hui",
    booktitle = "Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022",
    month = nov,
    year = "2022",
    address = "Online only",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2022.findings-aacl.23",
    pages = "237--245",
    abstract = "Translation of structured content is an important application of machine translation, but the scarcity of evaluation data sets, especially for Asian languages, limits progress. In this paper we present a novel multilingual multiway evaluation data set for the translation of structured documents of the Asian languages Japanese, Korean and Chinese. We describe the data set, its creation process and important characteristics, followed by establishing and evaluating baselines using the direct translation as well as detag-project approaches. Our data set is well suited for multilingual evaluation, and it contains richer annotation tag sets than existing data sets. Our results show that massively multilingual translation models like M2M-100 and mBART-50 perform surprisingly well despite not being explicitly trained to handle structured content. The data set described in this paper and used in our experiments is released publicly.",
}

@proceedings{wat-2021-asian,
    title = "Proceedings of the 8th Workshop on Asian Translation (WAT2021)",
    editor = "Nakazawa, Toshiaki  and
      Nakayama, Hideki  and
      Goto, Isao  and
      Mino, Hideya  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Kunchukuttan, Anoop  and
      Higashiyama, Shohei  and
      Manabe, Hiroshi  and
      Pa, Win Pa  and
      Parida, Shantipriya  and
      Bojar, Ond{\v{r}}ej  and
      Chu, Chenhui  and
      Eriguchi, Akiko  and
      Abe, Kaori  and
      Oda, Yusuke  and
      Sudoh, Katsuhito  and
      Kurohashi, Sadao  and
      Bhattacharyya, Pushpak",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2021.wat-1.0",
}

@inproceedings{nakazawa-etal-2021-overview,
    title = "Overview of the 8th Workshop on {A}sian Translation",
    author = "Nakazawa, Toshiaki  and
      Nakayama, Hideki  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Higashiyama, Shohei  and
      Mino, Hideya  and
      Goto, Isao  and
      Pa Pa, Win  and
      Kunchukuttan, Anoop  and
      Parida, Shantipriya  and
      Bojar, Ond{\v{r}}ej  and
      Chu, Chenhui  and
      Eriguchi, Akiko  and
      Abe, Kaori  and
      Oda, Yusuke  and
      Kurohashi, Sadao",
    editor = "Nakazawa, Toshiaki  and
      Nakayama, Hideki  and
      Goto, Isao  and
      Mino, Hideya  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Kunchukuttan, Anoop  and
      Higashiyama, Shohei  and
      Manabe, Hiroshi  and
      Pa, Win Pa  and
      Parida, Shantipriya  and
      Bojar, Ond{\v{r}}ej  and
      Chu, Chenhui  and
      Eriguchi, Akiko  and
      Abe, Kaori  and
      Oda, Yusuke  and
      Sudoh, Katsuhito  and
      Kurohashi, Sadao  and
      Bhattacharyya, Pushpak",
    booktitle = "Proceedings of the 8th Workshop on Asian Translation (WAT2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2021.wat-1.1",
    doi = "10.18653/v1/2021.wat-1.1",
    pages = "1--45",
    abstract = "This paper presents the results of the shared tasks from the 8th workshop on Asian translation (WAT2021). For the WAT2021, 28 teams participated in the shared tasks and 24 teams submitted their translation results for the human evaluation. We also accepted 5 research papers. About 2,100 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated.",
}

@inproceedings{dabre-chakrabarty-2021-nict,
    title = "{NICT}-5{'}s Submission To {WAT} 2021: {MBART} Pre-training And In-Domain Fine Tuning For Indic Languages",
    author = "Dabre, Raj  and
      Chakrabarty, Abhisek",
    editor = "Nakazawa, Toshiaki  and
      Nakayama, Hideki  and
      Goto, Isao  and
      Mino, Hideya  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Kunchukuttan, Anoop  and
      Higashiyama, Shohei  and
      Manabe, Hiroshi  and
      Pa, Win Pa  and
      Parida, Shantipriya  and
      Bojar, Ond{\v{r}}ej  and
      Chu, Chenhui  and
      Eriguchi, Akiko  and
      Abe, Kaori  and
      Oda, Yusuke  and
      Sudoh, Katsuhito  and
      Kurohashi, Sadao  and
      Bhattacharyya, Pushpak",
    booktitle = "Proceedings of the 8th Workshop on Asian Translation (WAT2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2021.wat-1.23",
    doi = "10.18653/v1/2021.wat-1.23",
    pages = "198--204",
    abstract = "In this paper we describe our submission to the multilingual Indic language translation wtask {``}MultiIndicMT{''} under the team name {``}NICT-5{''}. This task involves translation from 10 Indic languages into English and vice-versa. The objective of the task was to explore the utility of multilingual approaches using a variety of in-domain and out-of-domain parallel and monolingual corpora. Given the recent success of multilingual NMT pre-training we decided to explore pre-training an MBART model on a large monolingual corpus collection covering all languages in this task followed by multilingual fine-tuning on small in-domain corpora. Firstly, we observed that a small amount of pre-training followed by fine-tuning on small bilingual corpora can yield large gains over when pre-training is not used. Furthermore, multilingual fine-tuning leads to further gains in translation quality which significantly outperforms a very strong multilingual baseline that does not rely on any pre-training.",
}

@inproceedings{dabre-fujita-2021-investigating,
    title = "Investigating Softmax Tempering for Training Neural Machine Translation Models",
    author = "Dabre, Raj  and
      Fujita, Atsushi",
    editor = "Duh, Kevin  and
      Guzm{\'a}n, Francisco",
    booktitle = "Proceedings of Machine Translation Summit XVIII: Research Track",
    month = aug,
    year = "2021",
    address = "Virtual",
    publisher = "Association for Machine Translation in the Americas",
    html = "https://aclanthology.org/2021.mtsummit-research.10",
    pages = "114--126",
    abstract = "Neural machine translation (NMT) models are typically trained using a softmax cross-entropy loss where the softmax distribution is compared against the gold labels. In low-resource scenarios and NMT models tend to perform poorly because the model training quickly converges to a point where the softmax distribution computed using logits approaches the gold label distribution. Although label smoothing is a well-known solution to address this issue and we further propose to divide the logits by a temperature coefficient greater than one and forcing the softmax distribution to be smoother during training. This makes it harder for the model to quickly over-fit. In our experiments on 11 language pairs in the low-resource Asian Language Treebank dataset and we observed significant improvements in translation quality. Our analysis focuses on finding the right balance of label smoothing and softmax tempering which indicates that they are orthogonal methods. Finally and a study of softmax entropies and gradients reveal the impact of our method on the internal behavior of our NMT models.",
}

@inproceedings{dabre-etal-2021-studying,
    title = "Studying The Impact Of Document-level Context On Simultaneous Neural Machine Translation",
    author = "Dabre, Raj  and
      Imankulova, Aizhan  and
      Kaneko, Masahiro",
    editor = "Duh, Kevin  and
      Guzm{\'a}n, Francisco",
    booktitle = "Proceedings of Machine Translation Summit XVIII: Research Track",
    month = aug,
    year = "2021",
    address = "Virtual",
    publisher = "Association for Machine Translation in the Americas",
    html = "https://aclanthology.org/2021.mtsummit-research.17",
    pages = "202--214",
    abstract = "In a real-time simultaneous translation setting and neural machine translation (NMT) models start generating target language tokens from incomplete source language sentences and making them harder to translate and leading to poor translation quality. Previous research has shown that document-level NMT and comprising of sentence and context encoders and a decoder and leverages context from neighboring sentences and helps improve translation quality. In simultaneous translation settings and the context from previous sentences should be even more critical. To this end and in this paper and we propose wait-k simultaneous document-level NMT where we keep the context encoder as it is and replace the source sentence encoder and target language decoder with their wait-k equivalents. We experiment with low and high resource settings using the ALT and OpenSubtitles2018 corpora and where we observe minor improvements in translation quality. We then perform an analysis of the translations obtained using our models by focusing on sentences that should benefit from the context where we found out that the model does and in fact and benefit from context but is unable to effectively leverage it and especially in a low-resource setting. This shows that there is a need for further innovation in the way useful context is identified and leveraged.",
}

@proceedings{wat-2020-asian,
    title = "Proceedings of the 7th Workshop on Asian Translation",
    editor = "Nakazawa, Toshiaki  and
      Nakayama, Hideki  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Kunchukuttan, Anoop  and
      Pa, Win Pa  and
      Bojar, Ond{\v{r}}ej  and
      Parida, Shantipriya  and
      Goto, Isao  and
      Mino, Hidaya  and
      Manabe, Hiroshi  and
      Sudoh, Katsuhito  and
      Kurohashi, Sadao  and
      Bhattacharyya, Pushpak",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2020.wat-1.0",
}

@inproceedings{nakazawa-etal-2020-overview,
    title = "Overview of the 7th Workshop on {A}sian Translation",
    author = "Nakazawa, Toshiaki  and
      Nakayama, Hideki  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Higashiyama, Shohei  and
      Mino, Hideya  and
      Goto, Isao  and
      Pa Pa, Win  and
      Kunchukuttan, Anoop  and
      Parida, Shantipriya  and
      Bojar, Ond{\v{r}}ej  and
      Kurohashi, Sadao",
    editor = "Nakazawa, Toshiaki  and
      Nakayama, Hideki  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Kunchukuttan, Anoop  and
      Pa, Win Pa  and
      Bojar, Ond{\v{r}}ej  and
      Parida, Shantipriya  and
      Goto, Isao  and
      Mino, Hidaya  and
      Manabe, Hiroshi  and
      Sudoh, Katsuhito  and
      Kurohashi, Sadao  and
      Bhattacharyya, Pushpak",
    booktitle = "Proceedings of the 7th Workshop on Asian Translation",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2020.wat-1.1",
    pages = "1--44",
    abstract = "This paper presents the results of the shared tasks from the 7th workshop on Asian translation (WAT2020). For the WAT2020, 20 teams participated in the shared tasks and 14 teams submitted their translation results for the human evaluation. We also received 12 research paper submissions out of which 7 were accepted. About 500 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated.",
}

@inproceedings{dabre-chakrabarty-2020-nicts,
    title = "{NICT}{`}s Submission To {WAT} 2020: How Effective Are Simple Many-To-Many Neural Machine Translation Models?",
    author = "Dabre, Raj  and
      Chakrabarty, Abhisek",
    editor = "Nakazawa, Toshiaki  and
      Nakayama, Hideki  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Kunchukuttan, Anoop  and
      Pa, Win Pa  and
      Bojar, Ond{\v{r}}ej  and
      Parida, Shantipriya  and
      Goto, Isao  and
      Mino, Hidaya  and
      Manabe, Hiroshi  and
      Sudoh, Katsuhito  and
      Kurohashi, Sadao  and
      Bhattacharyya, Pushpak",
    booktitle = "Proceedings of the 7th Workshop on Asian Translation",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2020.wat-1.9",
    pages = "98--102",
    abstract = "In this paper we describe our team{`}s (NICT-5) Neural Machine Translation (NMT) models whose translations were submitted to shared tasks of the 7th Workshop on Asian Translation. We participated in the Indic language multilingual sub-task as well as the NICT-SAP multilingual multi-domain sub-task. We focused on naive many-to-many NMT models which gave reasonable translation quality despite their simplicity. Our observations are twofold: (a.) Many-to-many models suffer from a lack of consistency where the translation quality for some language pairs is very good but for some others it is terrible when compared against one-to-many and many-to-one baselines. (b.) Oversampling smaller corpora does not necessarily give the best translation quality for the language pair associated with that pair.",
}

@inproceedings{dabre-etal-2020-balancing,
    title = "Balancing Cost and Benefit with Tied-Multi Transformers",
    author = "Dabre, Raj  and
      Rubino, Raphael  and
      Fujita, Atsushi",
    editor = "Birch, Alexandra  and
      Finch, Andrew  and
      Hayashi, Hiroaki  and
      Heafield, Kenneth  and
      Junczys-Dowmunt, Marcin  and
      Konstas, Ioannis  and
      Li, Xian  and
      Neubig, Graham  and
      Oda, Yusuke",
    booktitle = "Proceedings of the Fourth Workshop on Neural Generation and Translation",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2020.ngt-1.3",
    doi = "10.18653/v1/2020.ngt-1.3",
    pages = "24--34",
    abstract = "We propose a novel procedure for training multiple Transformers with tied parameters which compresses multiple models into one enabling the dynamic choice of the number of encoder and decoder layers during decoding. In training an encoder-decoder model, typically, the output of the last layer of the N-layer encoder is fed to the M-layer decoder, and the output of the last decoder layer is used to compute loss. Instead, our method computes a single loss consisting of NxM losses, where each loss is computed from the output of one of the M decoder layers connected to one of the N encoder layers. Such a model subsumes NxM models with different number of encoder and decoder layers, and can be used for decoding with fewer than the maximum number of encoder and decoder layers. Given our flexible tied model, we also address to a-priori selection of the number of encoder and decoder layers for faster decoding, and explore recurrent stacking of layers and knowledge distillation for model compression. We present a cost-benefit analysis of applying the proposed approaches for neural machine translation and show that they reduce decoding costs while preserving translation quality.",
}

@inproceedings{song-etal-2020-coursera,
    title = "{C}oursera Corpus Mining and Multistage Fine-Tuning for Improving Lectures Translation",
    author = "Song, Haiyue  and
      Dabre, Raj  and
      Fujita, Atsushi  and
      Kurohashi, Sadao",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    html = "https://aclanthology.org/2020.lrec-1.449",
    pages = "3640--3649",
    abstract = "Lectures translation is a case of spoken language translation and there is a lack of publicly available parallel corpora for this purpose. To address this, we examine a framework for parallel corpus mining which is a quick and effective way to mine a parallel corpus from publicly available lectures at Coursera. Our approach determines sentence alignments, relying on machine translation and cosine similarity over continuous-space sentence representations. We also show how to use the resulting corpora in a multistage fine-tuning based domain adaptation for high-quality lectures translation. For Japanese{--}English lectures translation, we extracted parallel data of approximately 40,000 lines and created development and test sets through manual filtering for benchmarking translation performance. We demonstrate that the mined corpus greatly enhances the quality of translation when used in conjunction with out-of-domain parallel corpora via multistage training. This paper also suggests some guidelines to gather and clean corpora, mine parallel sentences, address noise in the mined data, and create high-quality evaluation splits. For the sake of reproducibility, we have released our code for parallel data creation.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{mao-etal-2020-jass,
    title = "{JASS}: {J}apanese-specific Sequence to Sequence Pre-training for Neural Machine Translation",
    author = "Mao, Zhuoyuan  and
      Cromieres, Fabien  and
      Dabre, Raj  and
      Song, Haiyue  and
      Kurohashi, Sadao",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    html = "https://aclanthology.org/2020.lrec-1.454",
    pages = "3683--3691",
    abstract = "Neural machine translation (NMT) needs large parallel corpora for state-of-the-art translation quality. Low-resource NMT is typically addressed by transfer learning which leverages large monolingual or parallel corpora for pre-training. Monolingual pre-training approaches such as MASS (MAsked Sequence to Sequence) are extremely effective in boosting NMT quality for languages with small parallel corpora. However, they do not account for linguistic information obtained using syntactic analyzers which is known to be invaluable for several Natural Language Processing (NLP) tasks. To this end, we propose JASS, Japanese-specific Sequence to Sequence, as a novel pre-training alternative to MASS for NMT involving Japanese as the source or target language. JASS is joint BMASS (Bunsetsu MASS) and BRSS (Bunsetsu Reordering Sequence to Sequence) pre-training which focuses on Japanese linguistic units called bunsetsus. In our experiments on ASPEC Japanese{--}English and News Commentary Japanese{--}Russian translation we show that JASS can give results that are competitive with if not better than those given by MASS. Furthermore, we show for the first time that joint MASS and JASS pre-training gives results that significantly surpass the individual methods indicating their complementary nature. We will release our code, pre-trained models and bunsetsu annotated data as resources for researchers to use in their own NLP tasks.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{song-etal-2020-pre,
    title = "Pre-training via Leveraging Assisting Languages for Neural Machine Translation",
    author = "Song, Haiyue  and
      Dabre, Raj  and
      Mao, Zhuoyuan  and
      Cheng, Fei  and
      Kurohashi, Sadao  and
      Sumita, Eiichiro",
    editor = "Rijhwani, Shruti  and
      Liu, Jiangming  and
      Wang, Yizhong  and
      Dror, Rotem",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2020.acl-srw.37",
    doi = "10.18653/v1/2020.acl-srw.37",
    pages = "279--285",
    abstract = "Sequence-to-sequence (S2S) pre-training using large monolingual data is known to improve performance for various S2S NLP tasks. However, large monolingual corpora might not always be available for the languages of interest (LOI). Thus, we propose to exploit monolingual corpora of other languages to complement the scarcity of monolingual corpora for the LOI. We utilize script mapping (Chinese to Japanese) to increase the similarity (number of cognates) between the monolingual corpora of helping languages and LOI. An empirical case study of low-resource Japanese-English neural machine translation (NMT) reveals that leveraging large Chinese and French monolingual corpora can help overcome the shortage of Japanese and English monolingual corpora, respectively, for S2S pre-training. Using only Chinese and French monolingual corpora, we were able to improve Japanese-English translation quality by up to 8.5 BLEU in low-resource scenarios.",
}

@inproceedings{dabre-fujita-2020-combining,
    title = "Combining Sequence Distillation and Transfer Learning for Efficient Low-Resource Neural Machine Translation Models",
    author = "Dabre, Raj  and
      Fujita, Atsushi",
    editor = {Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Graham, Yvette  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo},
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2020.wmt-1.61",
    pages = "492--502",
    abstract = "In neural machine translation (NMT), sequence distillation (SD) through creation of distilled corpora leads to efficient (compact and fast) models. However, its effectiveness in extremely low-resource (ELR) settings has not been well-studied. On the other hand, transfer learning (TL) by leveraging larger helping corpora greatly improves translation quality in general. This paper investigates a combination of SD and TL for training efficient NMT models for ELR settings, where we utilize TL with helping corpora twice: once for distilling the ELR corpora and then during compact model training. We experimented with two ELR settings: Vietnamese{--}English and Hindi{--}English from the Asian Language Treebank dataset with 18k training sentence pairs. Using the compact models with 40{\%} smaller parameters trained on the distilled ELR corpora, greedy search achieved 3.6 BLEU points improvement in average while reducing 40{\%} of decoding time. We also confirmed that using both the distilled ELR and helping corpora in the second round of TL further improves translation quality. Our work highlights the importance of stage-wise application of SD and TL for efficient NMT modeling for ELR settings.",
}

@inproceedings{kanojia-etal-2020-harnessing,
    title = "Harnessing Cross-lingual Features to Improve Cognate Detection for Low-resource Languages",
    author = "Kanojia, Diptesh  and
      Dabre, Raj  and
      Dewangan, Shubham  and
      Bhattacharyya, Pushpak  and
      Haffari, Gholamreza  and
      Kulkarni, Malhar",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    html = "https://aclanthology.org/2020.coling-main.119",
    doi = "10.18653/v1/2020.coling-main.119",
    pages = "1384--1395",
    abstract = "Cognates are variants of the same lexical form across different languages; for example {``}fonema{''} in Spanish and {``}phoneme{''} in English are cognates, both of which mean {``}a unit of sound{''}. The task of automatic detection of cognates among any two languages can help downstream NLP tasks such as Cross-lingual Information Retrieval, Computational Phylogenetics, and Machine Translation. In this paper, we demonstrate the use of cross-lingual word embeddings for detecting cognates among fourteen Indian Languages. Our approach introduces the use of context from a knowledge graph to generate improved feature representations for cognate detection. We, then, evaluate the impact of our cognate detection mechanism on neural machine translation (NMT), as a downstream task. We evaluate our methods to detect cognates on a challenging dataset of twelve Indian languages, namely, Sanskrit, Hindi, Assamese, Oriya, Kannada, Gujarati, Tamil, Telugu, Punjabi, Bengali, Marathi, and Malayalam. Additionally, we create evaluation datasets for two more Indian languages, Konkani and Nepali. We observe an improvement of up to 18{\%} points, in terms of F-score, for cognate detection. Furthermore, we observe that cognates extracted using our method help improve NMT quality by up to 2.76 BLEU. We also release our code, newly constructed datasets and cross-lingual models publicly.",
}

@inproceedings{chakrabarty-etal-2020-improving,
    title = "Improving Low-Resource {NMT} through Relevance Based Linguistic Features Incorporation",
    author = "Chakrabarty, Abhisek  and
      Dabre, Raj  and
      Ding, Chenchen  and
      Utiyama, Masao  and
      Sumita, Eiichiro",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    html = "https://aclanthology.org/2020.coling-main.376",
    doi = "10.18653/v1/2020.coling-main.376",
    pages = "4263--4274",
    abstract = "In this study, linguistic knowledge at different levels are incorporated into the neural machine translation (NMT) framework to improve translation quality for language pairs with extremely limited data. Integrating manually designed or automatically extracted features into the NMT framework is known to be beneficial. However, this study emphasizes that the relevance of the features is crucial to the performance. Specifically, we propose two methods, 1) self relevance and 2) word-based relevance, to improve the representation of features for NMT. Experiments are conducted on translation tasks from English to eight Asian languages, with no more than twenty thousand sentences for training. The proposed methods improve translation quality for all tasks by up to 3.09 BLEU points. Discussions with visualization provide the explainability of the proposed methods where we show that the relevance methods provide weights to features thereby enhancing their impact on low-resource machine translation.",
}

@inproceedings{dabre-etal-2020-multilingual,
    title = "Multilingual Neural Machine Translation",
    author = "Dabre, Raj  and
      Chu, Chenhui  and
      Kunchukuttan, Anoop",
    editor = "Specia, Lucia  and
      Beck, Daniel",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics: Tutorial Abstracts",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee for Computational Linguistics",
    html = "https://aclanthology.org/2020.coling-tutorials.3",
    doi = "10.18653/v1/2020.coling-tutorials.3",
    pages = "16--21",
    abstract = "The advent of neural machine translation (NMT) has opened up exciting research in building multilingual translation systems i.e. translation models that can handle more than one language pair. Many advances have been made which have enabled (1) improving translation for low-resource languages via transfer learning from high resource languages; and (2) building compact translation models spanning multiple languages. In this tutorial, we will cover the latest advances in NMT approaches that leverage multilingualism, especially to enhance low-resource translation. In particular, we will focus on the following topics: modeling parameter sharing for multi-way models, massively multilingual models, training protocols, language divergence, transfer learning, zero-shot/zero-resource learning, pivoting, multilingual pre-training and multi-source translation.",
}

@inproceedings{dabre-etal-2019-exploiting,
    title = "Exploiting Multilingualism through Multistage Fine-Tuning for Low-Resource Neural Machine Translation",
    author = "Dabre, Raj  and
      Fujita, Atsushi  and
      Chu, Chenhui",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/D19-1146",
    doi = "10.18653/v1/D19-1146",
    pages = "1410--1416",
    abstract = "This paper highlights the impressive utility of multi-parallel corpora for transfer learning in a one-to-many low-resource neural machine translation (NMT) setting. We report on a systematic comparison of multistage fine-tuning configurations, consisting of (1) pre-training on an external large (209k{--}440k) parallel corpus for English and a helping target language, (2) mixed pre-training or fine-tuning on a mixture of the external and low-resource (18k) target parallel corpora, and (3) pure fine-tuning on the target parallel corpora. Our experiments confirm that multi-parallel corpora are extremely useful despite their scarcity and content-wise redundancy thus exhibiting the true power of multilingualism. Even when the helping target language is not one of the target languages of our concern, our multistage fine-tuning can give 3{--}9 BLEU score gains over a simple one-to-one model.",
}

@proceedings{emnlp-2019-asian,
    title = "Proceedings of the 6th Workshop on Asian Translation",
    editor = "Nakazawa, Toshiaki  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Kunchukuttan, Anoop  and
      Doi, Nobushige  and
      Oda, Yusuke  and
      Bojar, Ond{\v{r}}ej  and
      Parida, Shantipriya  and
      Goto, Isao  and
      Mino, Hidaya",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/D19-5200",
}

@inproceedings{nakazawa-etal-2019-overview,
    title = "Overview of the 6th Workshop on {A}sian Translation",
    author = "Nakazawa, Toshiaki  and
      Doi, Nobushige  and
      Higashiyama, Shohei  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Mino, Hideya  and
      Goto, Isao  and
      Pa, Win Pa  and
      Kunchukuttan, Anoop  and
      Oda, Yusuke  and
      Parida, Shantipriya  and
      Bojar, Ond{\v{r}}ej  and
      Kurohashi, Sadao",
    editor = "Nakazawa, Toshiaki  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Kunchukuttan, Anoop  and
      Doi, Nobushige  and
      Oda, Yusuke  and
      Bojar, Ond{\v{r}}ej  and
      Parida, Shantipriya  and
      Goto, Isao  and
      Mino, Hidaya",
    booktitle = "Proceedings of the 6th Workshop on Asian Translation",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/D19-5201",
    doi = "10.18653/v1/D19-5201",
    pages = "1--35",
    abstract = "This paper presents the results of the shared tasks from the 6th workshop on Asian translation (WAT2019) including Ja↔En, Ja↔Zh scientific paper translation subtasks, Ja↔En, Ja↔Ko, Ja↔En patent translation subtasks, Hi↔En, My↔En, Km↔En, Ta↔En mixed domain subtasks and Ru↔Ja news commentary translation task. For the WAT2019, 25 teams participated in the shared tasks. We also received 10 research paper submissions out of which 61 were accepted. About 400 translation results were submitted to the automatic evaluation server, and selected submis- sions were manually evaluated.",
}

@inproceedings{dabre-sumita-2019-nicts,
    title = "{NICT}{'}s participation to {WAT} 2019: Multilingualism and Multi-step Fine-Tuning for Low Resource {NMT}",
    author = "Dabre, Raj  and
      Sumita, Eiichiro",
    editor = "Nakazawa, Toshiaki  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Kunchukuttan, Anoop  and
      Doi, Nobushige  and
      Oda, Yusuke  and
      Bojar, Ond{\v{r}}ej  and
      Parida, Shantipriya  and
      Goto, Isao  and
      Mino, Hidaya",
    booktitle = "Proceedings of the 6th Workshop on Asian Translation",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/D19-5207",
    doi = "10.18653/v1/D19-5207",
    pages = "76--80",
    abstract = "In this paper we describe our submissions to WAT 2019 for the following tasks: English{--}Tamil translation and Russian{--}Japanese translation. Our team,{``}NICT-5{''}, focused on multilingual domain adaptation and back-translation for Russian{--}Japanese translation and on simple fine-tuning for English{--}Tamil translation . We noted that multi-stage fine tuning is essential in leveraging the power of multilingualism for an extremely low-resource language like Russian{--}Japanese. Furthermore, we can improve the performance of such a low-resource language pair by exploiting a small but in-domain monolingual corpus via back-translation. We managed to obtain second rank in both tasks for all translation directions.",
}

@inproceedings{dabre-etal-2019-nicts,
    title = "{NICT}{'}s Supervised Neural Machine Translation Systems for the {WMT}19 News Translation Task",
    author = "Dabre, Raj  and
      Chen, Kehai  and
      Marie, Benjamin  and
      Wang, Rui  and
      Fujita, Atsushi  and
      Utiyama, Masao  and
      Sumita, Eiichiro",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/W19-5313",
    doi = "10.18653/v1/W19-5313",
    pages = "168--174",
    abstract = "In this paper, we describe our supervised neural machine translation (NMT) systems that we developed for the news translation task for Kazakh↔English, Gujarati↔English, Chinese↔English, and English→Finnish translation directions. We focused on leveraging multilingual transfer learning and back-translation for the extremely low-resource language pairs: Kazakh↔English and Gujarati↔English translation. For the Chinese↔English translation, we used the provided parallel data augmented with a large quantity of back-translated monolingual data to train state-of-the-art NMT systems. We then employed techniques that have been proven to be most effective, such as back-translation, fine-tuning, and model ensembling, to generate the primary submissions of Chinese↔English. For English→Finnish, our submission from WMT18 remains a strong baseline despite the increase in parallel corpora for this year{'}s task.",
}

@inproceedings{dabre-sumita-2019-nicts-supervised,
    title = "{NICT}{'}s Supervised Neural Machine Translation Systems for the {WMT}19 Translation Robustness Task",
    author = "Dabre, Raj  and
      Sumita, Eiichiro",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/W19-5362",
    doi = "10.18653/v1/W19-5362",
    pages = "533--536",
    abstract = "In this paper we describe our neural machine translation (NMT) systems for Japanese↔English translation which we submitted to the translation robustness task. We focused on leveraging transfer learning via fine tuning to improve translation quality. We used a fairly well established domain adaptation technique called Mixed Fine Tuning (MFT) (Chu et. al., 2017) to improve translation quality for Japanese↔English. We also trained bi-directional NMT models instead of uni-directional ones as the former are known to be quite robust, especially in low-resource scenarios. However, given the noisy nature of the in-domain training data, the improvements we obtained are rather modest.",
}

@inproceedings{marie-etal-2019-nicts-machine,
    title = "{NICT}{'}s Machine Translation Systems for the {WMT}19 Similar Language Translation Task",
    author = "Marie, Benjamin  and
      Dabre, Raj  and
      Fujita, Atsushi",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/W19-5428",
    doi = "10.18653/v1/W19-5428",
    pages = "208--212",
    abstract = "This paper presents the NICT{'}s participation in the WMT19 shared Similar Language Translation Task. We participated in the Spanish-Portuguese task. For both translation directions, we prepared state-of-the-art statistical (SMT) and neural (NMT) machine translation systems. Our NMT systems with the Transformer architecture were trained on the provided parallel data enlarged with a large quantity of back-translated monolingual data. Our primary submission to the task is the result of a simple combination of our SMT and NMT systems. According to BLEU, our systems were ranked second and third respectively for the Portuguese-to-Spanish and Spanish-to-Portuguese translation directions. For contrastive experiments, we also submitted outputs generated with an unsupervised SMT system.",
}

@inproceedings{imankulova-etal-2019-exploiting,
    title = "Exploiting Out-of-Domain Parallel Data through Multilingual Transfer Learning for Low-Resource Neural Machine Translation",
    author = "Imankulova, Aizhan  and
      Dabre, Raj  and
      Fujita, Atsushi  and
      Imamura, Kenji",
    editor = "Forcada, Mikel  and
      Way, Andy  and
      Haddow, Barry  and
      Sennrich, Rico",
    booktitle = "Proceedings of Machine Translation Summit XVII: Research Track",
    month = aug,
    year = "2019",
    address = "Dublin, Ireland",
    publisher = "European Association for Machine Translation",
    html = "https://aclanthology.org/W19-6613",
    pages = "128--139",
}

@inproceedings{nakazawa-etal-2018-overview,
    title = "Overview of the 5th Workshop on {A}sian Translation",
    author = "Nakazawa, Toshiaki  and
      Sudoh, Katsuhito  and
      Higashiyama, Shohei  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Mino, Hideya  and
      Goto, Isao  and
      Pa, Win Pa  and
      Kunchukuttan, Anoop  and
      Kurohashi, Sadao",
    editor = "Politzer-Ahles, Stephen  and
      Hsu, Yu-Yin  and
      Huang, Chu-Ren  and
      Yao, Yao",
    booktitle = "Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation: 5th Workshop on Asian Translation: 5th Workshop on Asian Translation",
    month = "1{--}3 " # dec,
    year = "2018",
    address = "Hong Kong",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/Y18-3001",
}

@inproceedings{dabre-etal-2018-nicts,
    title = "{NICT}{'}s Participation in {WAT} 2018: Approaches Using Multilingualism and Recurrently Stacked Layers",
    author = "Dabre, Raj  and
      Kunchukuttan, Anoop  and
      Fujita, Atsushi  and
      Sumita, Eiichiro",
    editor = "Politzer-Ahles, Stephen  and
      Hsu, Yu-Yin  and
      Huang, Chu-Ren  and
      Yao, Yao",
    booktitle = "Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation: 5th Workshop on Asian Translation: 5th Workshop on Asian Translation",
    month = "1{--}3 " # dec,
    year = "2018",
    address = "Hong Kong",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/Y18-3003",
}

@inproceedings{cromieres-etal-2017-kyoto,
    title = "{K}yoto {U}niversity Participation to {WAT} 2017",
    author = "Cromieres, Fabien  and
      Dabre, Raj  and
      Nakazawa, Toshiaki  and
      Kurohashi, Sadao",
    editor = "Nakazawa, Toshiaki  and
      Goto, Isao",
    booktitle = "Proceedings of the 4th Workshop on {A}sian Translation ({WAT}2017)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    html = "https://aclanthology.org/W17-5714",
    pages = "146--153",
    abstract = "We describe here our approaches and results on the WAT 2017 shared translation tasks. Following our good results with Neural Machine Translation in the previous shared task, we continue this approach this year, with incremental improvements in models and training methods. We focused on the ASPEC dataset and could improve the state-of-the-art results for Chinese-to-Japanese and Japanese-to-Chinese translations.",
}

@inproceedings{dabre-etal-2017-enabling,
    title = "Enabling Multi-Source Neural Machine Translation By Concatenating Source Sentences In Multiple Languages",
    author = "Dabre, Raj  and
      Cromieres, Fabien  and
      Kurohashi, Sadao",
    editor = "Kurohashi, Sadao  and
      Fung, Pascale",
    booktitle = "Proceedings of Machine Translation Summit XVI: Research Track",
    month = sep # " 18 {--} " # sep # " 22",
    year = "2017",
    address = "Nagoya Japan",
    html = "https://aclanthology.org/2017.mtsummit-papers.8",
    pages = "96--107",
}

@inproceedings{dabre-etal-2017-kyoto,
    title = "{K}yoto {U}niversity {MT} System Description for {IWSLT} 2017",
    author = "Dabre, Raj  and
      Cromieres, Fabien  and
      Kurohashi, Sadao",
    editor = "Sakti, Sakriani  and
      Utiyama, Masao",
    booktitle = "Proceedings of the 14th International Conference on Spoken Language Translation",
    month = dec # " 14-15",
    year = "2017",
    address = "Tokyo, Japan",
    publisher = "International Workshop on Spoken Language Translation",
    html = "https://aclanthology.org/2017.iwslt-1.8",
    pages = "55--59",
    abstract = "We describe here our Machine Translation (MT) model and the results we obtained for the IWSLT 2017 Multilingual Shared Task. Motivated by Zero Shot NMT [1] we trained a Multilingual Neural Machine Translation by combining all the training data into one single collection by appending the tokens to the source sentences in order to indicate the target language they should be translated to. We observed that even in a low resource situation we were able to get translations whose quality surpass the quality of those obtained by Phrase Based Statistical Machine Translation by several BLEU points. The most surprising result we obtained was in the zero shot setting for Dutch-German and Italian-Romanian where we observed that despite using no parallel corpora between these language pairs, the NMT model was able to translate between these languages and the translations were either as good as or better (in terms of BLEU) than the non zero resource setting. We also verify that the NMT models that use feed forward layers and self attention instead of recurrent layers are extremely fast in terms of training which is useful in a NMT experimental setting.",
}

@inproceedings{cromieres-etal-2017-neural,
    title = "Neural Machine Translation: Basics, Practical Aspects and Recent Trends",
    author = "Cromieres, Fabien  and
      Nakazawa, Toshiaki  and
      Dabre, Raj",
    editor = "Kurohashi, Sadao  and
      Strube, Michael",
    booktitle = "Proceedings of the {IJCNLP} 2017, Tutorial Abstracts",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    html = "https://aclanthology.org/I17-5004",
    pages = "11--13",
    abstract = "Machine Translation (MT) is a sub-field of NLP which has experienced a number of paradigm shifts since its inception. Up until 2014, Phrase Based Statistical Machine Translation (PBSMT) approaches used to be the state of the art. In late 2014, Neural Machine Translation (NMT) was introduced and was proven to outperform all PBSMT approaches by a significant margin. Since then, the NMT approaches have undergone several transformations which have pushed the state of the art even further. This tutorial is primarily aimed at researchers who are either interested in or are fairly new to the world of NMT and want to obtain a deep understanding of NMT fundamentals. Because it will also cover the latest developments in NMT, it should also be useful to attendees with some experience in NMT.",
}

@inproceedings{chu-etal-2017-empirical,
    title = "An Empirical Comparison of Domain Adaptation Methods for Neural Machine Translation",
    author = "Chu, Chenhui  and
      Dabre, Raj  and
      Kurohashi, Sadao",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/P17-2061",
    doi = "10.18653/v1/P17-2061",
    pages = "385--391",
    abstract = "In this paper, we propose a novel domain adaptation method named {``}mixed fine tuning{''} for neural machine translation (NMT). We combine two existing approaches namely fine tuning and multi domain NMT. We first train an NMT model on an out-of-domain parallel corpus, and then fine tune it on a parallel corpus which is a mix of the in-domain and out-of-domain corpora. All corpora are augmented with artificial tags to indicate specific domains. We empirically compare our proposed method against fine tuning and multi domain methods and discuss its benefits and shortcomings.",
}

@inproceedings{dabre-etal-2017-empirical,
    title = "An Empirical Study of Language Relatedness for Transfer Learning in Neural Machine Translation",
    author = "Dabre, Raj  and
      Nakagawa, Tetsuji  and
      Kazawa, Hideto",
    editor = "Roxas, Rachel Edita",
    booktitle = "Proceedings of the 31st Pacific Asia Conference on Language, Information and Computation",
    month = nov,
    year = "2017",
    publisher = "The National University (Phillippines)",
    html = "https://aclanthology.org/Y17-1038",
    pages = "282--286",
}

@inproceedings{dabre-etal-2016-kyoto,
    title = "The {K}yoto {U}niversity Cross-Lingual Pronoun Translation System",
    author = "Dabre, Raj  and
      Puzikov, Yevgeniy  and
      Cromieres, Fabien  and
      Kurohashi, Sadao",
    editor = {Bojar, Ond{\v{r}}ej  and
      Buck, Christian  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Guillou, Liane  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Pecina, Pavel  and
      Popel, Martin  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      Post, Matt  and
      Specia, Lucia  and
      Verspoor, Karin  and
      Tiedemann, J{\"o}rg  and
      Turchi, Marco},
    booktitle = "Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/W16-2349",
    doi = "10.18653/v1/W16-2349",
    pages = "571--575",
}

@inproceedings{chu-etal-2016-parallel,
    title = "Parallel Sentence Extraction from Comparable Corpora with Neural Network Features",
    author = "Chu, Chenhui  and
      Dabre, Raj  and
      Kurohashi, Sadao",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    html = "https://aclanthology.org/L16-1468",
    pages = "2931--2935",
    abstract = "Parallel corpora are crucial for machine translation (MT), however they are quite scarce for most language pairs and domains. As comparable corpora are far more available, many studies have been conducted to extract parallel sentences from them for MT. In this paper, we exploit the neural network features acquired from neural MT for parallel sentence extraction. We observe significant improvements for both accuracy in sentence extraction and MT performance.",
}

@inproceedings{kanojia-etal-2016-sophisticated,
    title = "Sophisticated Lexical Databases - Simplified Usage: Mobile Applications and Browser Plugins For Wordnets",
    author = "Kanojia, Diptesh  and
      Dabre, Raj  and
      Bhattacharyya, Pushpak",
    editor = "Fellbaum, Christiane  and
      Vossen, Piek  and
      Mititelu, Verginica Barbu  and
      Forascu, Corina",
    booktitle = "Proceedings of the 8th Global WordNet Conference (GWC)",
    month = "27--30 " # jan,
    year = "2016",
    address = "Bucharest, Romania",
    publisher = "Global Wordnet Association",
    html = "https://aclanthology.org/2016.gwc-1.22",
    pages = "144--149",
    abstract = "India is a country with 22 officially recognized languages and 17 of these have WordNets, a crucial resource. Web browser based interfaces are available for these WordNets, but are not suited for mobile devices which deters people from effectively using this resource. We present our initial work on developing mobile applications and browser extensions to access WordNets for Indian Languages. Our contribution is two fold: (1) We develop mobile applications for the Android, iOS and Windows Phone OS platforms for Hindi, Marathi and Sanskrit WordNets which allow users to search for words and obtain more information along with their translations in English and other Indian languages. (2) We also develop browser extensions for English, Hindi, Marathi, and Sanskrit WordNets, for both Mozilla Firefox, and Google Chrome. We believe that such applications can be quite helpful in a classroom scenario, where students would be able to access the WordNets as dictionaries as well as lexical knowledge bases. This can help in overcoming the language barrier along with furthering language understanding.",
}

@inproceedings{dabre-etal-2015-leveraging,
    title = "Leveraging Small Multilingual Corpora for {SMT} Using Many Pivot Languages",
    author = "Dabre, Raj  and
      Cromieres, Fabien  and
      Kurohashi, Sadao  and
      Bhattacharyya, Pushpak",
    editor = "Mihalcea, Rada  and
      Chai, Joyce  and
      Sarkar, Anoop",
    booktitle = "Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = may # "{--}" # jun,
    year = "2015",
    address = "Denver, Colorado",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/N15-1125",
    doi = "10.3115/v1/N15-1125",
    pages = "1192--1202",
}

@inproceedings{richardson-etal-2015-kyotoebmt,
    title = "{K}yoto{EBMT} System Description for the 2nd Workshop on {A}sian Translation",
    author = "Richardson, John  and
      Dabre, Raj  and
      Chu, Chenhui  and
      Cromi{\`e}res, Fabien  and
      Nakazawa, Toshiaki  and
      Kurohashi, Sadao",
    editor = "Nakazawa, Toshiaki  and
      Mino, Hideya  and
      Goto, Isao  and
      Neubig, Graham  and
      Kurohashi, Sadao  and
      Sumita, Eiichiro",
    booktitle = "Proceedings of the 2nd Workshop on {A}sian Translation ({WAT}2015)",
    month = oct,
    year = "2015",
    address = "Kyoto, Japan",
    publisher = "Workshop on Asian Translation",
    html = "https://aclanthology.org/W15-5006",
    pages = "54--60",
}

@inproceedings{more-etal-2015-augmenting,
    title = "Augmenting Pivot based {SMT} with word segmentation",
    author = "More, Rohit  and
      Kunchukuttan, Anoop  and
      Bhattacharyya, Pushpak  and
      Dabre, Raj",
    editor = "Sharma, Dipti Misra  and
      Sangal, Rajeev  and
      Sherly, Elizabeth",
    booktitle = "Proceedings of the 12th International Conference on Natural Language Processing",
    month = dec,
    year = "2015",
    address = "Trivandrum, India",
    publisher = "NLP Association of India",
    html = "https://aclanthology.org/W15-5944",
    pages = "303--307",
}

@inproceedings{dabre-etal-2015-large,
    title = "Large-scale Dictionary Construction via Pivot-based Statistical Machine Translation with Significance Pruning and Neural Network Features",
    author = "Dabre, Raj  and
      Chu, Chenhui  and
      Cromieres, Fabien  and
      Nakazawa, Toshiaki  and
      Kurohashi, Sadao",
    editor = "Zhao, Hai",
    booktitle = "Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",
    month = oct,
    year = "2015",
    address = "Shanghai, China",
    html = "https://aclanthology.org/Y15-1033",
    pages = "289--297",
}

@inproceedings{kanojia-etal-2014-processing,
    title = "Do not do processing, when you can look up: Towards a Discrimination Net for {WSD}",
    author = "Kanojia, Diptesh  and
      Bhattacharyya, Pushpak  and
      Dabre, Raj  and
      Gunti, Siddhartha  and
      Shrivastava, Manish",
    editor = "Orav, Heili  and
      Fellbaum, Christiane  and
      Vossen, Piek",
    booktitle = "Proceedings of the Seventh Global {W}ordnet Conference",
    month = jan,
    year = "2014",
    address = "Tartu, Estonia",
    publisher = "University of Tartu Press",
    html = "https://aclanthology.org/W14-0126",
    pages = "194--200",
}


@inproceedings{dabre-etal-2014-tackling,
    title = "Tackling Close Cousins: Experiences In Developing Statistical Machine Translation Systems For {M}arathi And {H}indi",
    author = "Dabre, Raj  and
      Choudhari, Jyotesh  and
      Bhattacharyya, Pushpak",
    editor = "Sharma, Dipti Misra  and
      Sangal, Rajeev  and
      Pawar, Jyoti D.",
    booktitle = "Proceedings of the 11th International Conference on Natural Language Processing",
    month = dec,
    year = "2014",
    address = "Goa, India",
    publisher = "NLP Association of India",
    html = "https://aclanthology.org/W14-5103",
    pages = "11--19",
}

@inproceedings{dabre-etal-2014-anou,
    title = "Anou Tradir: Experiences In Building Statistical Machine Translation Systems For Mauritian Languages {--} Creole, {E}nglish, {F}rench",
    author = "Dabre, Raj  and
      Sukhoo, Aneerav  and
      Bhattacharyya, Pushpak",
    editor = "Sharma, Dipti Misra  and
      Sangal, Rajeev  and
      Pawar, Jyoti D.",
    booktitle = "Proceedings of the 11th International Conference on Natural Language Processing",
    month = dec,
    year = "2014",
    address = "Goa, India",
    publisher = "NLP Association of India",
    html = "https://aclanthology.org/W14-5113",
    pages = "82--88",
}

@inproceedings{kanojia-etal-2014-pacman,
    title = "{P}a{CM}an : Parallel Corpus Management Workbench",
    author = "Kanojia, Diptesh  and
      Shrivastava, Manish  and
      Dabre, Raj  and
      Bhattacharyya, Pushpak",
    editor = "Sharma, Dipti Misra  and
      Sangal, Rajeev  and
      Pawar, Jyoti D.",
    booktitle = "Proceedings of the 11th International Conference on Natural Language Processing",
    month = dec,
    year = "2014",
    address = "Goa, India",
    publisher = "NLP Association of India",
    html = "https://aclanthology.org/W14-5126",
    pages = "162--166",
}

@inproceedings{dabre-etal-2012-morphological,
    title = "Morphological Analyzer for Affix Stacking Languages: A Case Study of {M}arathi",
    author = "Dabre, Raj  and
      Amberkar, Archana  and
      Bhattacharyya, Pushpak",
    editor = "Kay, Martin  and
      Boitet, Christian",
    booktitle = "Proceedings of {COLING} 2012: Posters",
    month = dec,
    year = "2012",
    address = "Mumbai, India",
    publisher = "The COLING 2012 Organizing Committee",
    html = "https://aclanthology.org/C12-2023",
    pages = "225--234",
}
