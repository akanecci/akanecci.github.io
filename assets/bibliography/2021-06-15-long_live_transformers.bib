@inproceedings{Vaswani2017AttentionIA,
  title     = {Attention is All you Need},
  author    = {Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  booktitle = {NIPS},
  year      = {2017},
  url       = {https://arxiv.org/abs/1706.03762}
}

@misc{Kaiser2017AttentionNN,
  title     = {Attentional Neural Network Models},
  author    = {≈Åukasz Kaiser},
  publisher = {Pi School},
  year      = {2017},
  url       = {https://www.youtube.com/watch?v=rBCqOTEfxvg}
}

@misc{Google2017Transformer,
  title     = {Transformer: A Novel Neural Network Architecture for Language Understanding},
  publisher = {Google AI},
  year      = {2017},
  url       = {https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html}
}

@misc{Alammar2018Transformer,
  title     = {The Illustrated Transformer},
  author    = {Jay Alammar},
  year      = {2018},
  url       = {https://jalammar.github.io/illustrated-transformer/}
}

@misc{Harvard2018Transformer,
  title   = {The Annotated Transformer},
  publisher  = {Harvard NLP Group},
  year    = {2018},
  url     = {https://nlp.seas.harvard.edu/2018/04/03/attention.html}
}

@article{weng2018attention,
  title   = "Attention? Attention!",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io",
  year    = "2018",
  url     = "https://lilianweng.github.io/posts/2018-06-24-attention/"
}

@misc{positional_2019_stack_exchange,
  title  = {What is the positional encoding in the transformer model?},
  publisher = {Stack Exchange},
  year   = {2019},
  url    = {https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model}
}

@misc{positional_2019_kazemnejad,
  title  = {Transformer Architecture: The Positional Encoding},
  author = {Amirhossein Kazemnejad},
  year   = {2019},
  url    = {https://kazemnejad.com/blog/transformer_architecture_positional_encoding}
}

@article{Dai2019TransformerXLAL,
  title   = {Transformer-XL: Attentive Language Models beyond a Fixed-Length Context},
  author  = {Zihang Dai and Zhilin Yang and Yiming Yang and Jaime G. Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1901.02860},
  url     = {https://arxiv.org/abs/1901.02860}
}

@misc{Google2019TransformerXL,
  title  = {Transformer-XL: Unleashing the Potential of Attention Models},
  publisher = {Google AI},
  year   = {2019},
  url    = {https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html}
}

@article{Kitaev2020ReformerTE,
  title={Reformer: The Efficient Transformer},
  author={Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
  journal={ArXiv},
  year={2020},
  volume={abs/2001.04451},
  url={https://arxiv.org/abs/2001.04451}
}

@misc{Google2020Reformer,
  title  = {Reformer: The Efficient Transformer},
  publisher = {Google AI},
  year   = {2020},
  url    = {https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html}
}

@article{Katharopoulos2020TransformersAR,
  title={Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention},
  author={Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and Franccois Fleuret},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.16236},
  url={https://arxiv.org/abs/2006.16236}
}
